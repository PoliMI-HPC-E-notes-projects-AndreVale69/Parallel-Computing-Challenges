{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVevP8s8y3aS"
      },
      "source": [
        "# Parallel Computing - Challenge 2\n",
        "\n",
        "| Name and Surname      | ID       |\n",
        "| --------------------- | -------- |\n",
        "| Alberto Ondei         | 11098067 |\n",
        "| Abdullah Javed        | 10764782 |\n",
        "| Andrea Valentini      | 11010856 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fn0bmqCBykvb"
      },
      "source": [
        "## Implementation\n",
        "\n",
        "### Check `nvcc` (CUDA) compiler version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9vmH_z8x2UD",
        "outputId": "4c146685-ed07-4acd-a979-da2337bd1eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XvB-OzA0SV3"
      },
      "source": [
        "### Install a [nvcc plugin](https://pypi.org/project/nvcc4jupyter/) for python notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDRG-I6Q0XDa",
        "outputId": "83724b07-9a6c-4f7f-a9d9-0431a4926f17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nvcc4jupyter\n",
            "  Downloading nvcc4jupyter-1.2.1-py3-none-any.whl.metadata (5.1 kB)\n",
            "Downloading nvcc4jupyter-1.2.1-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOc2t8Aa1NM1"
      },
      "source": [
        "### Load nvcc plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em_7iBwB1RKV",
        "outputId": "92316c7d-098b-406d-9f75-7e82d5f25c08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpt6nnzdtl\".\n"
          ]
        }
      ],
      "source": [
        "%load_ext nvcc4jupyter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDPG3qRX1Zs4"
      },
      "source": [
        "### Select mask size and matrix sizes\n",
        "\n",
        "Select the mask size (`MASK_SIZE`). The values of the mask will be random.\n",
        "\n",
        "Select a matrix size (`MATRIX_HEIGHT`, `MATRIX_WIDTH`). The values of the matrix will be random.\n",
        "\n",
        "You can also use the environments:\n",
        "- `SEED` to use a random seed to duplicate the generation of a matrix.\n",
        "- `WARMUP` to do _n_ warm-ups, mainly for benchmark reasons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXBFlLIo1i6A",
        "outputId": "63a41fed-a96e-4978-d381-9a60bc0536eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: MASK_SIZE=3\n",
            "env: MATRIX_HEIGHT=1000\n",
            "env: MATRIX_WIDTH=1000\n",
            "env: WARMUP=15\n"
          ]
        }
      ],
      "source": [
        "%env MASK_SIZE=3\n",
        "%env MATRIX_HEIGHT=1000\n",
        "%env MATRIX_WIDTH=1000\n",
        "# %env SEED=100\n",
        "# %env WARMUP=15"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic execution\n",
        "\n",
        "Basic 2D convolution using the GPU."
      ],
      "metadata": {
        "id": "MRarwZ4KiLmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <cassert>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cuda.h>\n",
        "#include <fstream>\n",
        "\n",
        "#define BLOCK_WIDTH 32\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/**\n",
        " * Basic 2D convolution kernel (no tiling).\n",
        " * @param input Input matrix.\n",
        " * @param output Output matrix.\n",
        " * @param height Height of the input matrix.\n",
        " * @param width Width of the input matrix.\n",
        " * @param mask Mask matrix.\n",
        " * @param mask_width Width of the mask matrix.\n",
        " */\n",
        "__global__ void convolution_2d_basic_kernel(\n",
        "    const float * input,\n",
        "    float * output,\n",
        "    const int height,\n",
        "    const int width,\n",
        "    const float * mask,\n",
        "    const int mask_width\n",
        ") {\n",
        "    const int col = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    const int row = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    const int mask_radius = mask_width / 2;\n",
        "\n",
        "    if (row < height && col < width) {\n",
        "        float result = 0.0;\n",
        "        for (int i = -mask_radius; i <= mask_radius; i++) {\n",
        "            for (int j = -mask_radius; j <= mask_radius; j++) {\n",
        "                const int cur_row = row + i;\n",
        "                if (const int cur_col = col + j; cur_row >= 0 && cur_row < height && cur_col >= 0 && cur_col < width) {\n",
        "                    result += input[cur_row * width + cur_col] * mask[(i + mask_radius) * mask_width + (j + mask_radius)];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "        output[row * width + col] = result;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "/**\n",
        " * Print a matrix.\n",
        " * @param matrix Matrix to print.\n",
        " * @param height Height of the matrix.\n",
        " * @param width Width of the matrix.\n",
        " */\n",
        "void print_matrix(const float* matrix, const int height, const int width) {\n",
        "    if (matrix == nullptr) {\n",
        "        throw invalid_argument(\"Matrix cannot be null\");\n",
        "    }\n",
        "    for (int i = 0; i < height * width; ++i) {\n",
        "        printf(\"%f \", matrix[i]);\n",
        "        if (i % width == width - 1) {\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Create a matrix with random values between 0 and upper_bound.\n",
        " * @param result Result matrix.\n",
        " * @param rows Number of rows of the output matrix.\n",
        " * @param cols Number of cols of the output matrix.\n",
        " * @param lower_bound Lower limit for random number generation.\n",
        " * @param upper_bound Upper limit for random number generation.\n",
        " * @throw invalid_argument If mask is null.\n",
        " */\n",
        "void create_random_matrix(float *result, const int rows, const int cols, const int lower_bound, const int upper_bound) {\n",
        "    if (lower_bound > upper_bound) {\n",
        "        throw invalid_argument(\"Lower bound cannot be greater than upper bound\");\n",
        "    }\n",
        "    if (result == nullptr) {\n",
        "        throw invalid_argument(\"Result matrix cannot be null\");\n",
        "    }\n",
        "    // init bound\n",
        "    const int boundary = rows * cols;\n",
        "    // insert values\n",
        "    for (int i = 0; i < boundary; ++i)\n",
        "        result[i] = ((random() % upper_bound) + lower_bound);\n",
        "}\n",
        "\n",
        "/**\n",
        " * Create a constant matrix with a specific value.\n",
        " * @param result Constant matrix result.\n",
        " * @param rows Rows of the final matrix, it should be square.\n",
        " * @param cols Cols of the final matrix, it should be square.\n",
        " * @param value Value to fill the matrix.\n",
        " * @throw invalid_argument If result is null.\n",
        " */\n",
        "void create_constant_matrix(float *result, const int rows, const int cols, const float value) {\n",
        "    if (result == nullptr)\n",
        "        throw invalid_argument(\"Result matrix cannot be null\");\n",
        "    // init bound\n",
        "    const int boundary = rows * cols;\n",
        "    // insert values\n",
        "    for (int i = 0; i < boundary; ++i)\n",
        "        result[i] = value;\n",
        "}\n",
        "\n",
        "/**\n",
        " * Verify the result of the convolution operation using the CPU.\n",
        " * @param matrix Input matrix.\n",
        " * @param mask Mask matrix.\n",
        " * @param result Result matrix to verify.\n",
        " * @param height Height of the input matrix.\n",
        " * @param width Width of the input matrix.\n",
        " * @param mask_dim Dimension of the mask matrix.\n",
        " */\n",
        "void verify_result(\n",
        "    const float *matrix,\n",
        "    const float *mask,\n",
        "    const float *result,\n",
        "    const int height,\n",
        "    const int width,\n",
        "    const int mask_dim\n",
        ") {\n",
        "    if (matrix == nullptr || mask == nullptr || result == nullptr) {\n",
        "        throw invalid_argument(\"Matrix, mask, and result cannot be null\");\n",
        "    }\n",
        "    const int mask_offset = mask_dim / 2;\n",
        "    for (int i = 0; i < height; i++) {\n",
        "        for (int j = 0; j < width; j++) {\n",
        "            float expected_convolution = 0.0;\n",
        "            for (int k = 0; k < mask_dim; k++) {\n",
        "                for (int l = 0; l < mask_dim; l++) {\n",
        "                    const int r = i - mask_offset + k;\n",
        "                    if (const int c = j - mask_offset + l; r >= 0 && r < height && c >= 0 && c < width) {\n",
        "                        expected_convolution += matrix[r * width + c] * mask[k * mask_dim + l];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            const float convolution = result[i * width + j];\n",
        "            assert(convolution == expected_convolution);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    // init\n",
        "    int mask_width = 0, matrix_width = 0, matrix_height = 0;\n",
        "    int seed = 0, warmup = 0;\n",
        "\n",
        "    // try to get env variable about the matrix size and mask size\n",
        "    try {\n",
        "        mask_width = stoi(getenv(\"MASK_SIZE\"));\n",
        "        matrix_width = stoi(getenv(\"MATRIX_WIDTH\"));\n",
        "        matrix_height = stoi(getenv(\"MATRIX_HEIGHT\"));\n",
        "        if (!(mask_width > 0 && matrix_width > 0 && matrix_height > 0 && mask_width % 2 != 0)) {\n",
        "            throw invalid_argument(\"Invalid argument\");\n",
        "        }\n",
        "    } catch (...) {\n",
        "        printf(\"Error reading MASK_SIZE env variable; it must be an integer.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "    // try to get env variable about the seed\n",
        "    try {\n",
        "        seed = stoi(getenv(\"SEED\"));\n",
        "    } catch (...) {\n",
        "       printf(\"WARNING: SEED env variable not found; random values will be generated.\\n\\n\");\n",
        "    }\n",
        "    // try to get env variable about the warmup\n",
        "    try {\n",
        "        warmup = stoi(getenv(\"WARMUP\"));\n",
        "    } catch (...) {\n",
        "        printf(\"WARNING: WARMUP env variable not set, a single run will be performed.\\n\\n\");\n",
        "    }\n",
        "\n",
        "    // retrieve some info about the CUDA device\n",
        "    cudaGetDeviceCount(nullptr);\n",
        "    cudaDeviceProp prop{};\n",
        "    cudaGetDeviceProperties(&prop, 0);\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  max Blocks Per MultiProcessor: %d\\n\", prop.maxBlocksPerMultiProcessor);\n",
        "    printf(\"  max Threads Per MultiProcessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
        "    printf(\"  max Threads Per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
        "    printf(\"  num SM: %d\\n\", prop.multiProcessorCount);\n",
        "    printf(\"  num bytes sharedMem Per Block: %lu\\n\", prop.sharedMemPerBlock);\n",
        "    printf(\"  num bytes sharedMem Per Multiprocessor: %lu\\n\", prop.sharedMemPerMultiprocessor);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\", prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\", prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\", 2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "\n",
        "\n",
        "    /** Execution **/\n",
        "    // init\n",
        "    float naive_gpu_elapsed_time_ms;\n",
        "    float* input_m = static_cast<float *>(malloc(matrix_height * matrix_width * sizeof(float)));\n",
        "    float* mask = static_cast<float *>(malloc(mask_width * mask_width * sizeof(float)));\n",
        "    float* output_m = static_cast<float *>(malloc(matrix_width * matrix_height * sizeof(float)));\n",
        "\n",
        "    // populate\n",
        "    if (seed != 0) {\n",
        "        srand(seed);\n",
        "    }\n",
        "    create_random_matrix(input_m, matrix_height, matrix_width, 1, 100);\n",
        "    create_random_matrix(mask, mask_width, mask_width, 1, 5);\n",
        "\n",
        "    // =============================================== START CONVOLUTION ===============================================\n",
        "    // time event\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    unsigned int grid_rows = (matrix_height + BLOCK_WIDTH - 1) / BLOCK_WIDTH;\n",
        "    unsigned int grid_cols = (matrix_height + BLOCK_WIDTH- 1) /BLOCK_WIDTH;\n",
        "    dim3 dim_grid(grid_cols, grid_rows);\n",
        "    dim3 dim_block(BLOCK_WIDTH, BLOCK_WIDTH);\n",
        "\n",
        "\n",
        "    const size_t bytes_input_m = matrix_height * matrix_width * sizeof(int);\n",
        "    const size_t bytes_mask = mask_width * mask_width * sizeof(int);\n",
        "\n",
        "    // allocate memory in the device\n",
        "    float* cuda_input_m;\n",
        "    float* cuda_mask;\n",
        "    float* cuda_output_m;\n",
        "    cudaMalloc(reinterpret_cast<void **>(&cuda_input_m), bytes_input_m);\n",
        "    cudaMalloc(reinterpret_cast<void **>(&cuda_mask), bytes_mask);\n",
        "    cudaMalloc(reinterpret_cast<void **>(&cuda_output_m), bytes_input_m); // same bytes as input\n",
        "\n",
        "    // initialize memory in the device\n",
        "    cudaMemcpy(cuda_input_m, input_m, bytes_input_m, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(cuda_mask, mask, bytes_mask, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // warmup\n",
        "    for (int i = 0; i < warmup; ++i) {\n",
        "        convolution_2d_basic_kernel<<<dim_grid, dim_block>>>(\n",
        "            cuda_input_m, cuda_output_m,matrix_height, matrix_width,  cuda_mask, mask_width\n",
        "        );\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(start, nullptr);\n",
        "    convolution_2d_basic_kernel<<<dim_grid, dim_block>>>(\n",
        "        cuda_input_m, cuda_output_m,matrix_height, matrix_width,  cuda_mask, mask_width\n",
        "    );\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, nullptr);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(output_m, cuda_output_m, bytes_input_m, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n",
        "\n",
        "    printf(\"Check the result of the convolution operation using the CPU...\\n\");\n",
        "    verify_result(input_m, mask, output_m, matrix_height, matrix_width, mask_width);\n",
        "    printf(\"Verification passed!\\n\");\n",
        "\n",
        "    // debug: print the output matrix\n",
        "    // print_matrix(output_m, matrix_height, matrix_width);\n",
        "    printf(\"Time elapsed on naive GPU 2D-convolution of %dx%d (block %d): %f ms.\\n\\n\",\n",
        "            matrix_height, matrix_width, BLOCK_WIDTH, naive_gpu_elapsed_time_ms);\n",
        "\n",
        "    cudaFree(cuda_input_m);\n",
        "    cudaFree(cuda_mask);\n",
        "    cudaFree(cuda_output_m);\n",
        "\n",
        "    // ================================================ END CONVOLUTION ================================================\n",
        "\n",
        "    // free\n",
        "    free(input_m);\n",
        "    free(mask);\n",
        "    free(output_m);\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnGag_PDiCn9",
        "outputId": "36243afe-92bc-4751-b9af-dcef111f7082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  max Blocks Per MultiProcessor: 16\n",
            "  max Threads Per MultiProcessor: 1024\n",
            "  max Threads Per Block: 1024\n",
            "  num SM: 40\n",
            "  num bytes sharedMem Per Block: 49152\n",
            "  num bytes sharedMem Per Multiprocessor: 65536\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "Check the result of the convolution operation using the CPU...\n",
            "Verification passed!\n",
            "Time elapsed on naive GPU 2D-convolution of 1000x1000 (block 16): 0.122912 ms.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tiling execution\n",
        "\n",
        "The code is the same of the basic execution, but the main is different."
      ],
      "metadata": {
        "id": "3mFxRv3phukb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHwZdzc4jRyG",
        "outputId": "0107fd07-8fba-4b4d-d1a5-f324463254c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: SEED env variable not found; random values will be generated.\n",
            "\n",
            "WARNING: WARMUP env variable not set, a single run will be performed.\n",
            "\n",
            "Device Number: 0\n",
            "  Device name: Tesla T4\n",
            "  max Blocks Per MultiProcessor: 16\n",
            "  max Threads Per MultiProcessor: 1024\n",
            "  max Threads Per Block: 1024\n",
            "  num SM: 40\n",
            "  num bytes sharedMem Per Block: 49152\n",
            "  num bytes sharedMem Per Multiprocessor: 65536\n",
            "  Memory Clock Rate (KHz): 5001000\n",
            "  Memory Bus Width (bits): 256\n",
            "  Peak Memory Bandwidth (GB/s): 320.064000\n",
            "\n",
            "Check the result of the convolution operation using the CPU...\n",
            "Verification passed!\n",
            "Time elapsed on naive GPU 2D-convolution of 512x512 (block 32, tiling: 30): 0.272416 ms.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "%%cuda\n",
        "#include <cassert>\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cuda.h>\n",
        "#include <fstream>\n",
        "\n",
        "#define BLOCK_WIDTH 32\n",
        "\n",
        "using namespace std;\n",
        "\n",
        "/**\n",
        " * 2D convolution kernel with tiling.\n",
        " * It optimizes the convolution operation by using shared memory to store the input matrix tile,\n",
        " * reducing the number of global memory accesses.\n",
        " *\n",
        " * The kernel first loads a tile of the input matrix into shared memory,\n",
        " * then performs the convolution operation within the tile.\n",
        " *\n",
        " * @param cuda_input_m Input matrix.\n",
        " * @param mask Mask matrix.\n",
        " * @param cuda_output_m Output matrix.\n",
        " * @param height Height of the input matrix.\n",
        " * @param width Width of the input matrix.\n",
        " * @param mask_width Width of the mask matrix.\n",
        " * @param N_TILE_WIDTH Tile width.\n",
        " */\n",
        "__global__ void convolution_2D_tiled_kernel(\n",
        "    const float* cuda_input_m,\n",
        "    const float* __restrict__ mask,\n",
        "    float* cuda_output_m,\n",
        "    const size_t height,\n",
        "    const size_t width,\n",
        "    const size_t mask_width,\n",
        "    const int N_TILE_WIDTH\n",
        ") {\n",
        "    // shared memory\n",
        "    __shared__ float tile_shared_memory[BLOCK_WIDTH][BLOCK_WIDTH];\n",
        "\n",
        "    // init\n",
        "    const int tx = threadIdx.x;\n",
        "    const int ty = threadIdx.y;\n",
        "    const int n_row = blockIdx.y * N_TILE_WIDTH + ty;\n",
        "    const int n_col = blockIdx.x * N_TILE_WIDTH + tx;\n",
        "    const int m_row = n_row - mask_width / 2;\n",
        "    const int m_col = n_col - mask_width / 2;\n",
        "\n",
        "    // boundary condition\n",
        "    if(m_row >= 0 && m_row < height && m_col >= 0 && m_col < width) {\n",
        "        // load element from input matrix to shared memory in the respective tile position\n",
        "        tile_shared_memory[ty][tx] = cuda_input_m[m_row * width + m_col];\n",
        "    } else {\n",
        "        // avoid branch divergence\n",
        "        tile_shared_memory[ty][tx] = 0;\n",
        "    }\n",
        "\n",
        "    // barrier synchronization\n",
        "    __syncthreads();\n",
        "\n",
        "    // boundary condition to avoid out-of-bounds access, because we calculate only N_TILE_LENGTH elements\n",
        "    if(ty < N_TILE_WIDTH && tx < N_TILE_WIDTH && n_row < height && n_col < width)\n",
        "    {\n",
        "        // convolution result\n",
        "        float convolution_result = 0;\n",
        "        // calculate convolution result\n",
        "        for(int i = 0; i < mask_width; ++i) {\n",
        "            for(int j = 0; j < mask_width; ++j) {\n",
        "                convolution_result += mask[i * mask_width + j] * tile_shared_memory[ty + i][tx + j];\n",
        "            }\n",
        "        }\n",
        "        // save convolution result to output matrix (barrier synchronization not needed)\n",
        "        cuda_output_m[n_row * width + n_col] = convolution_result;\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "/**\n",
        " * Print a matrix.\n",
        " * @param matrix Matrix to print.\n",
        " * @param height Height of the matrix.\n",
        " * @param width Width of the matrix.\n",
        " */\n",
        "void print_matrix(const float* matrix, const int height, const int width) {\n",
        "    if (matrix == nullptr) {\n",
        "        throw invalid_argument(\"Matrix cannot be null\");\n",
        "    }\n",
        "    for (int i = 0; i < height * width; ++i) {\n",
        "        printf(\"%f \", matrix[i]);\n",
        "        if (i % width == width - 1) {\n",
        "            printf(\"\\n\");\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "/**\n",
        " * Create a matrix with random values between 0 and upper_bound.\n",
        " * @param result Result matrix.\n",
        " * @param rows Number of rows of the output matrix.\n",
        " * @param cols Number of cols of the output matrix.\n",
        " * @param lower_bound Lower limit for random number generation.\n",
        " * @param upper_bound Upper limit for random number generation.\n",
        " * @throw invalid_argument If mask is null.\n",
        " */\n",
        "void create_random_matrix(float *result, const int rows, const int cols, const int lower_bound, const int upper_bound) {\n",
        "    if (lower_bound > upper_bound) {\n",
        "        throw invalid_argument(\"Lower bound cannot be greater than upper bound\");\n",
        "    }\n",
        "    if (result == nullptr) {\n",
        "        throw invalid_argument(\"Result matrix cannot be null\");\n",
        "    }\n",
        "    // init bound\n",
        "    const int boundary = rows * cols;\n",
        "    // insert values\n",
        "    for (int i = 0; i < boundary; ++i)\n",
        "        result[i] = ((random() % upper_bound) + lower_bound);\n",
        "}\n",
        "\n",
        "/**\n",
        " * Create a constant matrix with a specific value.\n",
        " * @param result Constant matrix result.\n",
        " * @param rows Rows of the final matrix, it should be square.\n",
        " * @param cols Cols of the final matrix, it should be square.\n",
        " * @param value Value to fill the matrix.\n",
        " * @throw invalid_argument If result is null.\n",
        " */\n",
        "void create_constant_matrix(float *result, const int rows, const int cols, const float value) {\n",
        "    if (result == nullptr)\n",
        "        throw invalid_argument(\"Result matrix cannot be null\");\n",
        "    // init bound\n",
        "    const int boundary = rows * cols;\n",
        "    // insert values\n",
        "    for (int i = 0; i < boundary; ++i)\n",
        "        result[i] = value;\n",
        "}\n",
        "\n",
        "/**\n",
        " * Verify the result of the convolution operation using the CPU.\n",
        " * @param matrix Input matrix.\n",
        " * @param mask Mask matrix.\n",
        " * @param result Result matrix to verify.\n",
        " * @param height Height of the input matrix.\n",
        " * @param width Width of the input matrix.\n",
        " * @param mask_dim Dimension of the mask matrix.\n",
        " */\n",
        "void verify_result(\n",
        "    const float *matrix,\n",
        "    const float *mask,\n",
        "    const float *result,\n",
        "    const int height,\n",
        "    const int width,\n",
        "    const int mask_dim\n",
        ") {\n",
        "    if (matrix == nullptr || mask == nullptr || result == nullptr) {\n",
        "        throw invalid_argument(\"Matrix, mask, and result cannot be null\");\n",
        "    }\n",
        "    const int mask_offset = mask_dim / 2;\n",
        "    for (int i = 0; i < height; i++) {\n",
        "        for (int j = 0; j < width; j++) {\n",
        "            float expected_convolution = 0.0;\n",
        "            for (int k = 0; k < mask_dim; k++) {\n",
        "                for (int l = 0; l < mask_dim; l++) {\n",
        "                    const int r = i - mask_offset + k;\n",
        "                    if (const int c = j - mask_offset + l; r >= 0 && r < height && c >= 0 && c < width) {\n",
        "                        expected_convolution += matrix[r * width + c] * mask[k * mask_dim + l];\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "            const float convolution = result[i * width + j];\n",
        "            assert(convolution == expected_convolution);\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "int main() {\n",
        "    // init\n",
        "    int mask_width = 0, matrix_width = 0, matrix_height = 0;\n",
        "    int seed = 0, warmup = 0;\n",
        "\n",
        "    // try to get env variable about the matrix size and mask size\n",
        "    try {\n",
        "        mask_width = stoi(getenv(\"MASK_SIZE\"));\n",
        "        matrix_width = stoi(getenv(\"MATRIX_WIDTH\"));\n",
        "        matrix_height = stoi(getenv(\"MATRIX_HEIGHT\"));\n",
        "        if (!(mask_width > 0 && matrix_width > 0 && matrix_height > 0 && mask_width % 2 != 0)) {\n",
        "            throw invalid_argument(\"Invalid argument\");\n",
        "        }\n",
        "    } catch (...) {\n",
        "        printf(\"Error reading MASK_SIZE env variable; it must be an integer.\\n\");\n",
        "        return 1;\n",
        "    }\n",
        "    // try to get env variable about the seed\n",
        "    try {\n",
        "        seed = stoi(getenv(\"SEED\"));\n",
        "    } catch (...) {\n",
        "        printf(\"WARNING: SEED env variable not found; random values will be generated.\\n\\n\");\n",
        "    }\n",
        "    // try to get env variable about the warmup\n",
        "    try {\n",
        "        warmup = stoi(getenv(\"WARMUP\"));\n",
        "    } catch (...) {\n",
        "        printf(\"WARNING: WARMUP env variable not set, a single run will be performed.\\n\\n\");\n",
        "    }\n",
        "\n",
        "    // retrieve some info about the CUDA device\n",
        "    cudaGetDeviceCount(nullptr);\n",
        "    cudaDeviceProp prop{};\n",
        "    cudaGetDeviceProperties(&prop, 0);\n",
        "    printf(\"Device Number: %d\\n\", 0);\n",
        "    printf(\"  Device name: %s\\n\", prop.name);\n",
        "    printf(\"  max Blocks Per MultiProcessor: %d\\n\", prop.maxBlocksPerMultiProcessor);\n",
        "    printf(\"  max Threads Per MultiProcessor: %d\\n\", prop.maxThreadsPerMultiProcessor);\n",
        "    printf(\"  max Threads Per Block: %d\\n\", prop.maxThreadsPerBlock);\n",
        "    printf(\"  num SM: %d\\n\", prop.multiProcessorCount);\n",
        "    printf(\"  num bytes sharedMem Per Block: %lu\\n\", prop.sharedMemPerBlock);\n",
        "    printf(\"  num bytes sharedMem Per Multiprocessor: %lu\\n\", prop.sharedMemPerMultiprocessor);\n",
        "    printf(\"  Memory Clock Rate (KHz): %d\\n\", prop.memoryClockRate);\n",
        "    printf(\"  Memory Bus Width (bits): %d\\n\", prop.memoryBusWidth);\n",
        "    printf(\"  Peak Memory Bandwidth (GB/s): %f\\n\\n\", 2.0*prop.memoryClockRate*(prop.memoryBusWidth/8)/1.0e6);\n",
        "\n",
        "\n",
        "    /** Execution **/\n",
        "    // init\n",
        "    float naive_gpu_elapsed_time_ms;\n",
        "    const int N_TILE_WIDTH = BLOCK_WIDTH - (mask_width - 1);\n",
        "    float* input_m = static_cast<float *>(malloc(matrix_height * matrix_width * sizeof(float)));\n",
        "    float* mask = static_cast<float *>(malloc(mask_width * mask_width * sizeof(float)));\n",
        "    float* output_m = static_cast<float *>(malloc(matrix_width * matrix_height * sizeof(float)));\n",
        "\n",
        "    // populate\n",
        "    if (seed != 0) {\n",
        "        srand(seed);\n",
        "    }\n",
        "    create_random_matrix(input_m, matrix_height, matrix_width, 1, 100);\n",
        "    create_random_matrix(mask, mask_width, mask_width, 1, 5);\n",
        "\n",
        "    // =============================================== START CONVOLUTION ===============================================\n",
        "    // time event\n",
        "    cudaEvent_t start, stop;\n",
        "    cudaEventCreate(&start);\n",
        "    cudaEventCreate(&stop);\n",
        "\n",
        "    // define grid and block dimensions\n",
        "    dim3 dim_grid(\n",
        "        ceil(matrix_width / static_cast<float>(N_TILE_WIDTH)),\n",
        "        ceil(matrix_height / static_cast<float>(N_TILE_WIDTH))\n",
        "    );\n",
        "    dim3 dim_block(BLOCK_WIDTH, BLOCK_WIDTH);\n",
        "    const size_t bytes_input_m = matrix_height * matrix_width * sizeof(int);\n",
        "    const size_t bytes_mask = mask_width * mask_width * sizeof(int);\n",
        "\n",
        "    // allocate memory in the device\n",
        "    float* cuda_input_m;\n",
        "    float* cuda_mask;\n",
        "    float* cuda_output_m;\n",
        "    cudaMalloc(reinterpret_cast<void **>(&cuda_input_m), bytes_input_m);\n",
        "    cudaMalloc(reinterpret_cast<void **>(&cuda_mask), bytes_mask);\n",
        "    cudaMalloc(reinterpret_cast<void **>(&cuda_output_m), bytes_input_m); // same bytes as input\n",
        "\n",
        "    // initialize memory in the device\n",
        "    cudaMemcpy(cuda_input_m, input_m, bytes_input_m, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(cuda_mask, mask, bytes_mask, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // warmup\n",
        "    for (int i = 0; i < warmup; ++i) {\n",
        "        convolution_2D_tiled_kernel<<<dim_grid, dim_block>>>(\n",
        "            cuda_input_m, cuda_mask, cuda_output_m, matrix_height, matrix_width, mask_width, N_TILE_WIDTH\n",
        "        );\n",
        "    }\n",
        "\n",
        "    cudaEventRecord(start, nullptr);\n",
        "    convolution_2D_tiled_kernel<<<dim_grid, dim_block>>>(\n",
        "        cuda_input_m, cuda_mask, cuda_output_m, matrix_height, matrix_width, mask_width, N_TILE_WIDTH\n",
        "    );\n",
        "    cudaDeviceSynchronize();\n",
        "    cudaEventRecord(stop, nullptr);\n",
        "    cudaEventSynchronize(stop);\n",
        "\n",
        "    cudaMemcpy(output_m, cuda_output_m, bytes_input_m, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\n",
        "    cudaEventElapsedTime(&naive_gpu_elapsed_time_ms, start, stop);\n",
        "\n",
        "    printf(\"Check the result of the convolution operation using the CPU...\\n\");\n",
        "    verify_result(input_m, mask, output_m, matrix_height, matrix_width, mask_width);\n",
        "    printf(\"Verification passed!\\n\");\n",
        "\n",
        "    // debug: print the output matrix\n",
        "    // print_matrix(output_m, matrix_height, matrix_width);\n",
        "    printf(\"Time elapsed on naive GPU 2D-convolution of %dx%d (block %d): %f ms.\\n\\n\",\n",
        "        matrix_height, matrix_width, BLOCK_WIDTH, naive_gpu_elapsed_time_ms);\n",
        "\n",
        "    cudaFree(cuda_input_m);\n",
        "    cudaFree(cuda_mask);\n",
        "    cudaFree(cuda_output_m);\n",
        "\n",
        "    // ================================================ END CONVOLUTION ================================================\n",
        "\n",
        "    // free\n",
        "    free(input_m);\n",
        "    free(mask);\n",
        "    free(output_m);\n",
        "    return 0;\n",
        "}\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}